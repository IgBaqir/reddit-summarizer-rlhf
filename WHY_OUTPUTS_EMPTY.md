# Why is the Outputs Folder Empty?

## Explanation

The `outputs/` folder is **intentionally empty** until you run evaluation or training scripts. This is normal and expected behavior.

## What Goes in the Outputs Folder?

The `outputs/` folder stores:

1. **Evaluation Results** (`evaluation_results.json`)
   - Generated by running `evaluation/evaluate.py`
   - Contains ROUGE scores, reward scores, and other metrics
   - Only created after you evaluate a trained model

2. **Other Generated Outputs**
   - Any additional outputs from training/evaluation processes
   - Logs, reports, or analysis files

## How to Populate the Outputs Folder

### Option 1: Run Evaluation (Requires Trained Model)

```bash
# After training a model
python3 evaluation/evaluate.py \
    --model ./checkpoints/sft \
    --test-data ./data/splits/test.json \
    --output ./outputs/evaluation_results.json
```

This will create `outputs/evaluation_results.json` with:
- ROUGE-1, ROUGE-2, ROUGE-L scores
- Reward model scores (if reward model provided)
- Per-sample and average metrics

### Option 2: Run Full Training Pipeline

```bash
# This will eventually create evaluation results
./run_pipeline.sh
```

The pipeline script runs evaluation at the end, which populates the outputs folder.

### Option 3: Check Training Checkpoints

The actual model checkpoints are saved in:
- `checkpoints/sft/` - Supervised fine-tuned models
- `checkpoints/reward_model/` - Reward models
- `checkpoints/ppo/` - PPO-trained models

## Current Status

- ✅ `outputs/` folder exists (created during setup)
- ⏳ Waiting for evaluation to be run
- ⏳ Waiting for models to be trained

## Quick Test

I've created a sample output file to show you what the format looks like:

```bash
cat outputs/evaluation_results.json
```

This is just a sample. Real results will be generated when you run the evaluation script.

## Summary

**The outputs folder is empty because:**
1. No evaluation has been run yet
2. No trained models exist yet (checkpoints folder is also empty)
3. The folder was just created as part of project setup

**To populate it:**
1. Train a model first (SFT, Reward, or PPO)
2. Run the evaluation script
3. Results will be saved to `outputs/evaluation_results.json`

This is normal behavior - the folder exists and is ready to receive outputs when you run the evaluation!

